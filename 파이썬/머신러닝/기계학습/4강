분류 문제 기본 개념 및 규칙 기반 분류 모델



기계학습

지도학습 - 비지도 학습



분류

주요 데이터 클래스를추출해낼 수 있도록 분류 모델을 구성하는 데이터 분석 방법

분류 모델은 훈련 집합을 기반으로 연속적이고 순서가 없는 클래스 라벨 예측



예측

관측된 변수들을 기반으로 연속형 숫자 값을 예측하는 분석 방법

회귀분석(Regression Analysis)과 같은 방법을 사용함



분류 모델의 구성-> 분류 모델의 사용



step1 분류 모델의 구성

훈련 데이터 세트의 튜플마다 기존의 클래스 분류를 나타내는 클래스 라벨이 있음을 전제로함.

주어진 튜플 X에 대해 클래스 라벨y를 찾는 관계를 모델링함

모델링 결과 모델의 형태는 분류 규칙, 결정 나무 또는 수학적 공식의 형태



step2 분류 모델의 사용

모델을 통해 클래스 라벨을 예측하는 단계

분류에 적용한 모델의 정확성을 확인



분류 방법론

규칙 기반 모델, 의사 결정 나무, 나이브 베이지안 모델 등



앙상블

배깅, 부스팅, 랜덤포레스트



IF-THEN 규칙들로 구성된 학습 모델

IF age=youth AND student= yes

THEN buys_computer=yes



규칙의 평가

커버리지(Coverage)

Coverage(R)= 규칙R이 만족하는 갯수/전체 갯수



정확도(Accuracy)

Accuracy(R)=규칙R에 정확히 분류된 개수/규칙R이 커버하는 개수



경합해소 전략(conflict Resolution)

다수의 규칙 R1, R2에 의한 분류결과가 각기 다를 수 있음

모든규칙을 충족하지 않을 수가 있음



해결방법

규모정렬(size ordering)

까다로운 규칙에 최우선권 부여

IF부분에 해당하는 속성 테스트의 개수로 결정



규칙정렬(Rule Ordering)

해당 분야의 전문가의 조언에 따른 사전의 규칙 우선순위 결정



순차적 포괄 알고리즘(Sequential Covering Algorithm)

주어진 클래스에 대해 이상적으로 가장 많이 포괄하면서 다른 클래스는 포괄하지 않는다.









4.2 의사결정나무



결정 나무 유도(Decision Tree Induction)

:클래스 라벨이 있는 훈련 데이터를 바탕으로 결정 나무를 학습하는 기법






HomeOwner -> Yes 종료

No -> Marital state -> Married 종료

Single, Diverced -> 계속진행



그리디 알고리즘(Greedy Algorithm)

하향식(Top-Down)의 재귀적 분할정복 방식으로 결정 나무를 구성

다음 조건에서 하나라도 만족하면 분할 종료

1) 노드에서 데이터의 모든 튜플이 같은 클래스일 때

2) 추가로 튜플을 분류하기 위한 속성이 남지 않았을 때: 다수결의 원칙

3) 남은 튜플이 없을 때



의사 결정 나무에서 속성 분류시 활용하는 지표



엔트로피(정보이론)

랜덤변수와 연관된 불확실성의 척도

-> 높은 엔트로피는 높은 불확실성, 확률이 0.5에 가까울수록 높은 엔트로피값




정보 소득(Information Gain)

파티션 D에 포함된 속성 중 가장 정보 소득이 높은 속성을 노드 N의 구분 속성으로 선택

임의성이 가장 적고 파티션의 불순함이 가장 낮은 속성






소득률(gain ratio)

정보 소득은 많은 값을 가진 속성에 편향되기에 소득률을 계산해서, 소득률을 가장 높은 것을 구분 속성을 선택





지니계수(Gini Index)

파티션 전체의 불순도를 최소화하는 속성을 구분 속성으로 선택

지니 계수가 가장 낮은 속성




과적합(Overfitting)

테스트 데이터에 대해 낮은 정확성



해결방법

가지치기(Pruning)

1)미리치기(prepruning)

나무 구성을 일직 중단하는 방법

적절한 중단점을 찾기가 어렵다



2)나중치기(Postpruning)

다 만든 나무를 바탕으로 부분 가지를 제거하는 작업



결정 나무의 성능 향상

연속형 속성 -> 이산형 속성으로 분해

결측값-> 다른 값으로 대체

거의 등장하지 않는 속성->새로운 속성 생성



의사결정나무의 장점

단순하고 빠르다.

분류정확도가 훌륭

SQL쿼리만으로도 충분

직관적이고 이해가 쉽다.



단점

대용량 데이터로의 확장





레인포레스트(RainForest)

대규모 훈련 데이터에 대해 결정 나무를 구성할 수 있는 방법

AVC 세트의 형식으로 저장해서 노드에 연결된 훈련 튜플을 저장



속성(Attribute)-값(Value)-클래스의 레이블(Class Label)

AVC세트->AVC그룹





4.3 베이지안 분류 모델



베이지안 분류 모델



나이브 베이지안 분류 모델

:통계적인 분류 모델로 주어진 튜플이 특정 클래스에 들어갈 가능성을 계산해서 클래스 배정 확률 예측





전체확률의 법칙(Total probability Theory)

:조건부 확률과 사전 확률의 곱을 이용하여 사후 확률을 예측하는 방법




베이즈 이론

:베이즈 이론을 이용하여 데이터가 들어왔을 때 이 데이터가 클래스 C에 속할 확률




클래스에 대한 우도


우도를 구해 높은 쪽으로 분류를 한다.





확률이 0인 경우 어떻게 해야할까?



->라플라스 보정(Laplacian Correction)

모든 속성값에 튜플을 하나씩 더하기





베이지안 분류 모델 장단점

장점

구현이 쉽다

단점

실제 변수 사이에 의존성이 존재한다.
